# 应用系统体系架构

$$
Wang \ Haotian
$$

 ## 10.21 Security

微服务本身不在乎请求从哪里过来的，通过GateWay隔离之后请求没有差异

### Kerberos协议

交大财务调用认证中心，认证中心有两个功能：

1. 认证：你是谁（AS）
2. 授权：你能干什么（TGS）Ticker Granting Server



认证授权部署在同一个服务器上，只做单点认证



Kerbos协议的流程（PPT security第62张的图）

1. 用户想要访问交大财务，现在有一个认证中心的服务器
2. 用户在客户端输入UserId和Pwd
3. UserId发送给AS，Pwd留在本地经过hash产生一个key_user
4. 服务器段根据UserId找到一个user，经过相同的hash算法产生一个key_as
5. key不能在网络上传输的，若AS用key_as加密一个东西（session key，一段时间内有效），user可以拿自己的key_user解密，说明密码对了
6. AS认证之后要进行授权，需要和TGS交互，不能明文交互。AS产生一个session key，通过刚才AS生成的key进行加密，用户拿到session key之后和TGS交互，但是是否相互信任？看下变的流程
7. TGS用TGS的pub_key加密一个session key产生message_b
8. user不能解开message_b，则原封不动传回去，把session key和message_b发回去
9. session key用TGS_pubkey加密之后产生message_c，把UserId和timestamp用session key加密之后产生message_d
10. TGS拿自己的private key解密得到session key，再用session key解密message_d得到信息，知道你在什么时候想要访问
11. 接着TGS进入TGS的数据库查权限信息，有权限才接着进行下一步
12. TGS会生成一个新的key，用于user和交大财务交互
13. TGS用session key加密上边说的user和交大财务交互的key，称message_f，传给user，user就可以用session key解开然后拿到key
14. TGS生成message_e，用交大财务的pub_key加密user和交大财务交互的key，同时附带user client的IP等信息
15. user解密message_f之后，没有交大财务的private key，所以message_e解不开，把message_e原封不动的发回来，交大财务解开之后能拿到client的信息和密钥
16. 用户再发给交大财务client一个message_g，包含client信息，用user和交大财务交互的key加密。交大财务已经有了这个key，因此可以拿到信息
17. 由于TGS是用交大财务的pub_key加密的，所以这个肯定是一个真的网站，不是一个钓鱼网站
18. 交大财务用user和交大财务交互的key加密每次user发来的时间戳+1，称为message_h，client可以检查这个时间戳对不对，以验证交大财务server对不对。



为什么叫单点认证？

AS认证只做一次



局限性：

多跑几个AS和TGS分布式实例，不然就裂开



Kerberos三头狗：

Client、AS/TGS、Service Server相互制约

### 安全

常见攻击手段：

- 跨站脚本攻击：比如一个文本输入框里边输入脚本
- 注入式攻击：密码框输入必定为真的sql脚本
- 分布式拒绝服务攻击（DDOS）：脚本同时动作，发大量请求搞坏服务器



抵御用户的跨站脚本攻击：

1. 把脚本的符号转义掉
2. 富文本编辑器，当文本展示，不当脚本运行

防止注入式攻击：

1. 不要用statement拼sql字符串方式进行JDBC的访问

防止分布式拒绝服务攻击：

1. 同一个IP多少时间内限定访问（但是不同机器就不行了）
2. 做好访问检测



安全的手段：

1. 密文存储
2. 链路加密
3. 维护数据完整性（hash，checksum）
4. 代码有限暴露（尽量所有都是private），封装接口，多个接口合为一个
5. 配置防火墙
6. 入侵检测系统（抓所有的包进行检测）
7. 恢复，打ckpt
8. 日志工具



## 10.25 MySQL Opt

**首先记住一点，数据库的东西在硬盘上存，访问时一定要load到memory，内存比硬盘小，且存在数据不一致，这是一切复杂性的根源**



索引，数据库结构，表的结构，字段设置等

OLAP，在线分析处理，列存比较合适

OLTP，在线事务处理，行存比较合适



### 索引

索引建树的类型

- 索引优化：B+树

- 地理位置索引：R-tree，空间索引是一个地理位置的值，经纬度、海拔、投影值等等很多信息

- MEMORY table支持哈希索引（hash会占用空间）

- InnoDB反向索引实现搜索引擎



索引的注意事项

- 多列索引的排序很重要

- 在查找时，索引文件load到内存里，再内存中搜索，如果索引文件很大，还得把当前这一页换出去，会有额外空间、时间的开销

- 数据量很小的时候建索引，效果提升不明显

- 索引太长，可以做前缀索引。

- 在批量操作，几乎操作所有数据时，索引没有太大意义

- InnoDB在disk上存数据时是按照主键索引的顺序存储的

- 主键要保证绝对唯一，如果是多列联合主键，会增加复杂性



关于全局唯一标识符：

在服务器集群时，可以用全局唯一标识符例如UUID，防止自增主键的重复，UUID是32位16进制数，一共16字节。但是UUID占用空间比较大，而且不能知道插入的先后顺序，若需要时间信息，需要加入额外时间戳。



关于外键：

1. 给不常用的列可以进行分表，外键关联，便于每次向内存加载更多数据，每次不需要从硬盘加载大量数据。否则对常用列的操作性能会降低。
2. 主键要尽量简单，会用其他表的主键当外键



关于前缀索引：

MySQL会对数据做压缩，根据存储方式的不同，前缀索引的长度可以不同



关于多列复合索引：

1. 最多支持16个列，没什么特别限制
2. 查询时匹配的顺序时建索引时的顺序，跳过第一列直接搜索第二列，索引就废了
3. JPA使用数据库的速度可能没那么高，JPA翻译的sql语句可能和预想的不同



关于Hash索引：

不能做范围查找，但是单个查找非常快，order by是不行的



关于索引顺序：

- B+树索引，默认升序，也可以设置为倒序
- 在进行Order By操作时，如果和索引的顺序是相同的，就不需要排序，节省一点时间



### 数据库设计

数据尺寸：

- 数据量尽量小一点=>数据压缩
- 表的列
- 行的格式
- 索引
- Join操作
- 范式化（衡量数据的冗余度，范式化越高数据冗余越小，但是性能可能会变差，因为外键操作特别多，join操作也很多）



表的列：

- 适合它的最小类型
- 尽量不让列为null，可能的话生成not null，null不利于索引，同时占用额外空间，还不利于编程



行结构：

- 可以对行进行一点压缩/Compact（Compact不一定要用压缩算法去压缩，而是一种Compact的表示，在时序数据库中也有体现）



索引：

- 主键索引越短越好，因为外键要存
- 组合索引比多个单独索引要好，因为组合索引是一整个！插入操作不用调整很多个树
- 总的索引越少越好，否则增加插入开销



Join：

- 拆表做join，减少冗余



范式化：

- 去冗余
- 数据同步问题



## 10.28 MySQL Opt

### 数据类型选择

- 能用数字，不用字符串
- 文本内容很多，不如存成Text或Blob
- 行的尺寸有限制，小于8k可以用varchar，否则一定要用text或者blob，因为一页16k（假设是16k），带上其他数据之后，一行可能存不下
- 主键选择，当一行数据很多时，UUID的空间占用是可以接受的，但是UUID之间无法比较先后顺序
- blob可以设置懒加载，有关blob的表可以专门设置一台机器或者硬盘，和其他经常存取的数据区分开
- blob的比较可以，可以比较摘要，长字符串比较效率很低



### MySQL的设置相关

- table open cache

- 建表、建库的数量和文件系统有关，但是同时打开的表的数量与cache有关，超出上限会告诉你打开表的数量达到上限，暂时不能打开
- 打开的表是在后台统计的，通常比想象的多
- Tomcat启动时会创建数据连接池，MySQL为了使外部连接看起来统一，会为每个连接打开一张表，比如有四个并发会话连接，都打开book table时，就会打开四张表
- MySQL可以设置最大连接数，最大打开表的数量。不能开太多，会占用内存，提升复杂性
- 表只有在evict cache或者flush时才会被认为关闭，类似于内存页的换进换出
- 内存满了会换出最近最少使用的表（替换策略可以换）
- 而在流量很大时，可能会突破内存限制，用完之后立刻关掉（比如，双十一时最近最少使用的表也只是在100ms以内，evict掉很不合理）
- 当Union时，在内存中建一张临时表进行两张表的合并，合并之后在放到disk，这也是**打开表数量超出预想的值**的原因之一
- 表的尺寸不能太大，收到文件系统文件大小的限制
- 表尺寸过大，可以分表，逻辑上可以partition，或者物理上把文件数据分开放在分布式文件系统中
- MySQL列数量有上限
- 行的尺寸也有限制，65535 bytes = 64k，但也不一定，每行的尺寸必须小于页尺寸的一半上限65535是设置page size=128k的情况
- varchar有2byte表示占用长度，nullable需要额外部分尺寸表示是否为空，这两种可能导致行尺寸超出容量限制



### InnoDB

- Optimize storage layout，类似文件碎片整理，还能增加局部性
- 事务AutoCommit，可以多个写操作放在一个事务，关掉AutoCommit，不必每次flush
- long running tx的roll back，部分已经落盘，回滚有额外开销
- long running tx应该拆分成小的tx
- Cache放大一些
- 当tx里边全部是select，MySQL会进行优化（开启只读事务，隔离等级不一样，no-locking select statement）
- rollback要redo、undo，buffer开的大，可以记录操作结果，恢复很容易，比记录操作容易



导入大量数据：

- 关掉AutoCommit

- data loading有大量有insert，若开启AutoCommit，每次都要flush，开销大

- 唯一索引约束插入时会检查唯一性，关掉AutoCommit，会load在内存，内存中检查唯一性比硬盘检查更快
- 使用多行插入
- 整数递增主键间隔设置为2，多线程插入防止冲突



优化Query：

- 主键默认做索引，不要指定太多、太长的列
- 经常访问的列，建其他索引，而且是复合索引而不是多个单独索引
- 尽量声明not null



优化Disk I/O

- 内存不够必然在内存和硬盘换进换出，CPU占用率就高；CPU不忙时，没有频繁的换进换出，那这是怀疑Disk I/O时瓶颈
- 解决方案之一是扩大内存
- 还可以优化flush策略，减少flush
- 还可以指定脏页超过阈值就flush
- DMA直接硬盘访问，跳过内存
- 换硬盘！整一个非旋转性的（比如SSD）！
- 随机读取用SSD很快，而HDD就不太行，但是顺序读取时SSD和HDD性能其实差不多的。HDD保存数据时间比SSD更长。
- 当有500G的SSD和2TB的HDD，fusion storage融合存储，设计一些维度，把不同访问模式的数据放在不同的存储介质上



预抓取Read ahead：

- 缓存在从硬盘读一个表时，会把某一行数据所在的一小块load到缓存
- 后边也有可能接着在下一次读取，这个想法时make sense的
- 因此会预抓取，把下边的也读进来
- 不能一次预抓取太多了，要控制预抓取的数量，因为预抓取是基于判断的，猜错一次开销很大
- 预抓取的策略可以优化



删数据：

truncate table而不是delete，truncate直接抹除



## 11.01

### InnoDB Buffer

table open cache：存的是table handler

InnoDB buffer：存的是数据

buffer很多块，每个之间是独立的，就是很多buffer实例

每一个实例会有多少内存

buffer实例数x每个buffer的大小是总的大小，但是每块缓存的大小需要是128M的整数倍



为什么要有多个实例？提高并发性，不同线程不在同一个实例操作

必须有很大的内存才会分实例，否则不会分实例



访问数据时，首先去table-open-cache看能不能打开一个表，取table handler

然后再去InnoDB-buffer找data



buffer如果放不下了需要清除缓存，就会使用LRU的方法换出一块数据

但是如果是请求count或者sum，需要扫描全表，而且被换到内存之后，短时间又不会被访问，因此设置最热是不合理的

因此新的进来的数据不会是hottest，而是LRU策略排序的3/8的位置



缓存读取数据时，会进行optimize，减少碎片、增加有序性

既然数据已经是有序性的了，为什么不多读取一些？可以多读一点

随机的读一些数据，防止需要的时候才去硬盘找，减少开销



数据全部加载到buffer，会有写操作的，经常把数据write back到硬盘上去

一旦dirty页的数量超过阈值，就触发flush的动作，整个buffer的脏页写到硬盘上去

写完之后，数据仍然保存在内存里，只是说把bitmap的dirty bit清除掉

多线程进行flush，线程数可以设置，一般设置为instance的数量

可以将memory的数据周期性的写到硬盘，这样进行warm-up的时间就会减少，不用重新根据用户的读写重新慢慢load，而是一次性直接load 8G进来（不一定是8G，看buffer大小）



把cache分成多个区，指定哪些表只能缓存在哪些区（hot、warm、cold等区）

hot和cold都比较小，warm最大

都是缓存，hot和cold的cache有什么区别？

- 管理内存的方式应该是有差异的
- 存在hot => warm => cold的逐渐流动



加载时还要注意，索引加载到cache是加载所有的索引还是不加载叶子节点？

- 可以设置的

- 如果是UUID，加载叶子节点，会占用很大空间
- 可以不加载叶子节点，这样加载的索引就非常多，省一半空间，虽然读取时需要再加载，但是缓存会省很多，可以放更多的索引



如果要enlarge cache：

- 先写dirty page
- 然后把所有的缓存都删掉
- 重新分配一次内存
- 这样会导致重新构建缓存，和重启没区别



Prepare Statement：

- 把已经解析编译好的sql语句存下来
- 以后再来一个相似的语句，直接用，比较快



trigger：

如果我在user表有一个balance，但是在这一列上边有一个数据约束

当insert时会触发trigger检查是否符合constraint



存储过程：

- client通过tomcat访问mysql
- 如果没有存储过程，全都要在tomcat里边load然后计算，这样会很占用内存
- 但是如果在mysql写存储过程，靠近数据源，传输数据量少



### Backup& Recovery

逻辑备份还是物理备份？

全量备份还是增量备份？



逻辑 & 物理：

物理备份：

拷贝文件出来，好处是数据库很大时备份出来的东西小，而且不用转换成脚本，速度快，还可以备份log。但是可迁移性不好，必须是同一个版本的mysql，还会对硬件有要求，而且导入数据时，必须关掉mysql



逻辑备份：

生成脚本，可迁移性强，而且可以在运行时recover，粒度可以细粒度控制，删掉一些行也是可以的。但是导出的文件大，而且生成脚本，导出速度慢



备份是remote还是local？

如果mysqldump的方式生成脚本，是可以返回给服务器的

但是如果是生成特定格式分开的文件，是在数据库机器本地的



snapshot：

copy on write的方式，存储的很少，没有一次性直接copy

没有修改的数据是没有copy的，在以后恢复，是拿初始内容加上snapshot的内容拼接起来的数据

少量数据就可以记住某一个时间点的快照

快照是一个逻辑的副本，没有直接copy，不然要存储爆炸了



全量 & 增量：

全量备份：

直接进行简单的拷贝

在做完全量备份的时候，之前的bin log都可以删掉了，不然太占用空间

做全量备份时，要先锁表，flush，文件全部一致才可以进行拷贝



增量备份：

写入bin-log，存储的是sql语句

bin-log可以进行压缩和加密，就可以减少空间，防止被别人看见

 bin-log也是可以导出的



Recovery：

在恢复时，找到某一个时刻的全量备份，load到数据库中，然后重新执行bin-log，就可以恢复到之前的状态



备份时可以在slave上边进行备份，这样master不用关

如果表坏了，可以用repair命令



设计全量备份的策略：

- 定期dump全部的数据到一个文件并且标时间
- dump出来的是脚本而不是文件
- dump是一个事务，all or nothing
- dump要加锁



只要做了dump，就可以删掉全部的bin-log了

只有单个数据库的dump，不会带有create和use的命令，需要先手动建数据库

如果导出tab分割的文本文件，是可以用可视化工具打开的，比较直观



dump也不单单是数据，还会把存储过程、函数、trigger等dump出来

你可以选择是不是要把业务逻辑也dump出来

## 11.04

### 选择性地 Recover

如何有选择性的执行bin-log？假如有人运行了drop database，我们可以跳过这条命令，执行这条命令之前的全部bin-log，然后重新在执行这条bin-log之后的全部命令

但是如果是一条其他的命令，比如update，即使跳过也不一定能恢复正常逻辑



### 关于存储过程和函数

如果存在业务逻辑，可以把业务逻辑放在mysql，tomcat只需要把参数发过来然后运行，这样减少mysql和tomcat之间数据发送的总量；但是这样对数据库迁移不是很友好，还有可能产生兼容性的问题

如果把业务逻辑存在MySQL里边，性能会比Tomcat高

但是在MySQL中编写业务逻辑，对于程序的编写不友好



如何选择？需要做trade-off！

如果是大量的呈现式的业务，存在大量的数据传输，可以把逻辑写在MySQL中

否则尽量写在Tomcat服务器中



### Partition

如果一个表太大，大到放不下，需要进行分割，防止出现一个特别大的文件

MyISAM不支持，只能用InnoDB

sql语句只支持分表，至于说是不是存在不同的机器上边，这个需要数据库管理系统实现

vertical的切分需要用户自己做，水平的切割叫做分区

进行表的分区，一张表被分成了物理上不同的几个部分，删除某一个分区就会很快，甚至还可以指定对某一个分区进行查询



分区最重要的是分区的逻辑：

- 离散型，数据取值是离散的
- 连续型分区

按照条件分区，每个区之间不能有重叠

分区也不一定是要平衡的，分区的条件可以自己写

甚至可以写函数，按照函数计算的值进行分区



那么能不能在两列上边分区？（多个列）

可以！

多列分区，会按照每一列的条件进行判断，是按照tuple的比较

```
(1, 5) < (2, 3) # True
(2, 8) < (2, 6) # False
(1, 2) < (1, 2) # False
```

按索引依次比较对应元素
如不相等，则结果为元组比较的结果(如上第1行代码)
如相等，则比较下一对元素，直至有结果(如上第2行代码)
如所有对应元素都相等，则判为相等(如上第3行代码)



less than (5,12)

那么比较 (5,10)  (5,11)  (5,12) 前两个都是true，第三个是false



- 多列分区，会按照每一列的条件进行判断，是按照tuple的比较

- 还可以按照LIST分区，每一个LIST中的值在一个区。但是LIST分区，只承认在LSIT中出现的数据，如果找不到对应的，就会报错

- 还有哈希分区（hash partitioning）和线性哈希分区（linear hash partitioning），还支持自定义hash函数

- key partitioning键分区
- subpartitioning可以进一步细化



如果是range分区，null的值对应的是minvalue，一定会被分到最小的分区里边

如果是list分区，null需要单独处理，任何一个list都没有null就会报错

如果是hash和key，null会把它的值当作0计算



把一个分区删掉之后，数据也就没有了

删除分区之后再去插入数据，插入的位置可能会改变

分区会做搜索剪枝的操作，搜索限制在小的范围内，筛选掉了大量无关数据



如果需要加新的range或list分区：

- 在后边追加分区，不能在中间加新的分区
- reorganize才能在中间搞新的分区，其实就是整个重新组织了一遍



对于hash和key分区：

- 不能drop
- 可以merge
- 所有的数据都要留下来，但是分区的数量发生了变化，需要重新组织



T2复制表T1的结构，但是删掉分区

T1是分区的，可以把T1的某些分区交换到T2，这样T2就有了数据

对T2在加入一些数据，再交换回去，就不行了



## 11.08 NoSQL & MongoDB

NoSQL：not only sql

支持schema不严格的存储



大数据场景下，数据量大的问题：

- 数据能不能存的下
- 如果存的下，能不能快速读出来=>多个盘



100G的10个硬盘，可以每个人的100G数据存到10个硬盘上边，每个10G，这样等于说每个人的读写速率提升了10倍

数据组织成分布式存储，利用率没有降低，但是速度提高了

但是这种组织方式使得故障率提高，需要冗余来提高可靠性



当我们引入分布式存储，数据分布在多个硬盘上，每一部分处理完，合起来结果可能不是最终的结果



为什么在分布式场景下，关系型数据库不太行了？

- 数据太多，表太大的话性能不好
- 如果进行分表，或者分区，都会引起在多台机器、多个表之间的连接问题



NoSQL支持非结构化、半结构化的数据，有没有一个字段和字段可以为空时完全不同的概念



### MongoDB

MongoDB支持auto-sharding，因为它都是键值对，不存在连接问题

document是MongoDB的基本数据单位

document的集合就是collection，对应的是表

每一个document有一个special key "_id"，在collection中是独一无二的

但是一个collection的documents可以有不同的shape和type

document是许多键值对，可以放很多键值对，但是键值对是有序的

document的中的key大小写敏感，类型敏感，一个document中不能有相同的键



## 11.11 MongoDB

MongoDB是键值对，可以是异构的，没有相同的schema

可以查询某些document不存在而另一些document存在的的键



### Sharding

- MongoDB拿很多服务器存储，每一个服务器叫做shard server

- 有一个config记住collection被切成多少块，在哪些server上边

- MondoDB的collection被分成了chunk，每一个collection都可以被分布式地存在不同的shard server上边

- 可以根据ID分成不同的chunk，chunk存储在不同的shard server上边

- 分好范围之后，生成document是会生成ID，就知道应该放在哪一个chunk上边

- 当chunk超过限制的大小，chunk会分裂成两个

- 会保证所有shard server上边的chunk数量相差小于2，一旦超过，就会对chunk做迁移

- 因此保证负载均衡，如果指定chunk size 64MB，那么每一个shard server的负载差距不超过128MB

- 但是可以关掉这个chunk的自动迁移，在特殊场景下是很有用的



### 图数据库

在拿节点和边描述数据的结构

关系型数据库的多对多很难真是描述，而且需要做很多表和表之间的连接



怎么存？查询语言？

节点上打标签，就表明了类型

节点之间的边表示了节点之间的关系



Querying Graphs：Cypher语法

很复杂，摆烂了



搜索的过程就是按照边的结构特征进行遍历

关系型数据库是在做笛卡尔积，join操作，巨慢无比

图数据库可以做协同过滤：Alice喜欢一本书，看看喜欢这本书的人还喜欢什么，推荐给Alice



Neo4j可以变成jar跑在Tomcat里边，也可以单独跑一个服务器内嵌在server里边，嵌入式的。

每天启动一个，把数据存下来，不希望数据在一台服务器上，



### 图数据库的Data Model

第一个节点，下一个节点是谁，下一个属性是谁

类似于链表，用双向链表存储关系，比关系型数据库的join操作要快



## 11.15

### Neo4j应用

数据清洗

Neo4j切图，每一张图负责一部分

把图塞道神经网络里边

寻找洗钱网络是不能直接写sql语句的

需要构建集群，使用机器学习



### 混合存储的问题（行存储？列存储？）

行存适合OLTP场景：

- 保证加载一次就可以把数据load到内存里



列存适合OLAP场景：

- 经常需要拿到所有数据的某一个字段，这种情况下行存储需要把所有数据load到内存但是只取一个字段，有很大的overhead
- 如果数据太大，可以分块存储，这在HBase中可以体现



当需要同时支持OLTP和OLAP时怎么搞？有两种策略：

1. 数据存储两份，一份行存，一份列存
2. 数据只存储一份，如果事务型操作多就行存储，如果分析型数据多就列存储



### B树和B+树

B Tree 和 B+ Tree也是数据库的一种存储方式

B Tree中间节点也可以存放值

B+ Tree把数据全部放在叶子节点，只存key不存value。而且B+树的叶子节点有指向兄弟节点的指针，支持Range Query

但是插入操作可能导致连续的分裂，有空间放大率

而且存在很多空洞，对内存造成浪费



### log-structured merge tree

是kv-store，有增删改查的接口

分层结构，每一层是上一层的几倍大小

内存中有一层，读写都在是对内存的直接操作

为了防止崩溃，会有WAL

内存有两块，一块写满之后，立刻切换到另一块，同时满的那一块进行flush到L0

L0的key是有重叠的，但是在进行compaction时进行多路归并排序

L1之后的层都不能有key的重叠

bloom filter可以加速查找过程



由于LSMT新数据在上层，因此读热数据性能比较好



空间放大率：

- 基本没有，数据禁止排列的

时间放大率：

- 读放大：读的时候，时间放大率会比较高，如果找的时不存在的数据，会一直读到最后一层

- 写放大：写的时候，可能每一层都满了，就需要进行连续的compaction

有索引块，找到offset，因此字符串可以任意长

为了节省空间，不是对任意一个kv都存储，它会进行优化，可能是找出key额公共前缀进行分组存储



CRC循环校验码，如果按行存，每一行进行一个CRC的计算就可以

但是如果按列存，怎么知道每一行的数据是否是对的？

按列存，追加一些数据，CRC是不是会改变？

按行存，key有公共部分，可以存成一个，但是如果按列存，会比较复杂



### RocksDB

rocksDB默认按照行存

大量写操作进来，高并发，低延迟

数据统计，读操作，低并发、高延迟



#### rocksDB写阻塞问题

写流程：

优点：

- 低延迟插入
- 写入内存直接返回
- 后台进行异步的写入硬盘

缺点：

- L0写满时将阻塞内存到磁盘的flush的过程
- L0下沉，compaction过程无法多任务执行
- 异步写，写放大严重，容易磁盘变成瓶颈
- 降低了transaction processing的可用性

优化：

- 中间层缓存数据（多加内存，满了立刻切换？）
- 负载均衡



#### 读放大问题

需要访问所有可能的数据文件

解决方案：

- 新列存结构
- 在线混合存储决策



### 常见的列式存储

Row Group格式

行列折中格式



常见的行列混合存储决策算法：

- 行列对等存储
- 行列差异存储
- 基于查询的分析



## 11.18 时序数据库

时间序列数据库：数据上带有时间戳

如果用关系型数据库

| id   | timestamp       | price | account |
| ---- | --------------- | ----- | ------- |
|      | xxxxxxxxxxxxxxx |       |         |
|      |                 |       |         |

时间戳占用空间太大

时间戳不能当主键，可能会有同一时刻很多条记录

而且时间戳当主键导致索引太长

如果数据库内的数据过多，老旧的数据还要不要？存活多长时间？



如果是时序数据库，像是一个环形队列，满了之后会删掉最旧的数据

时序数据库的特点：

- 数据压缩之后，相比于关系型数据库可以存放更多的数据
- 数据都比较简单，没有relation



### InfluxDB

- InfluxDB内有很多Bucket，每一个bucket是同一个结构
- bucket第一列是时间戳，这是必须有的，时间非常精确
- measurement，即对数据做一下分类，对bucket中的数据分组之后描述一下是干嘛的
- tag，tag上边会建索引。tag也有tag key和tag value
- field有key和value，field上边不建索引。从field set是同一个时间点上采集到的所有数据的集合。采集到的数据一定是以field的方式存储，但是field不做索引，如果一个field经常使用，需要把它转成tag



为什么field不能建立索引？

和数据的存储方式有关，field这种存储方式没法建立索引

| 时序数据库中的field | 关系型数据库 |
| ------------------- | ------------ |
| 100                 | 100          |
| +1                  | 101          |
| -1                  | 100          |

tag和field之间是可以相互转换的



能不能有多个tag？最好不要

比如既有bees又有ants，但是某一时刻来的数据只有bees，没有ants，就没法放了。

他们其实不能放在一个bucket里边，放在两个bucket中是比较好的



所谓时间序列是指针对某一个相同的东西，只是时间上不同，他们构成一个序列。而把ants和bees放在一起是没有意义的



InfluxDB和RocksDB类似，插入操作也是后台进行的，严格限制更新和删除

如果你进行删除和更新，就会停掉数据的插入，执行完query再接着插入数据（采样）



InfluxDB如果数据向下落，就说明是老旧数据，会进行压缩；而内存中的数据是不会进行压缩的

为了有比较好的性能，最好batch process

接收到数据后，组成一个batch再存入数据库，不要一条一条插入

一旦写入硬盘，旧的内存就被删掉了



InfluxDB是以列存储的方式进行存储：

- 利于数据压缩（看上边的例子）



序列的key要做索引，key是measurement + tag + field，索引不需要那么多，只有这一种



时序数据库的数据特别多，数据量巨大：

- 支持分布式，可以切成shard
- shard针对的是落在硬盘上的数据
- measuerment是逻辑上的东西，而shard是针对的物理上存储，分成不同的文件



默认情况下，只有一个shard，可以自行指定shard的存活时间

可以指定新的shard不压缩，旧的数据shard进行压缩

不能为过去的数据做shard

由于InfluxDB的数据时间戳是单调递增，因此数据不存在时间上的overlap



### 总结一下NoSQL

MongoDB

Neo4j

RocksDB & LevelDB

InfluxDB

Lucene & Solr & ElasticSearch



Dao把多个Repository组合起来拿到数据，Entity，本质还是TP（事务型处理）

如果要做AP，那么我们需要DataWarehouse

但是这么多数据库，怎么进行统一便捷的管理？DataLake



## 11.22 云原生数据库 & DataLake

### 云原生数据库

云原生数据库：未必是关系型数据库，还可以是非关系型数据库

云原生数据库，依托于云平台，结合了一些硬件设施进行优化



数据库在云上的特点：

- 虚拟化，资源的池化：目的是提高资源的利用率
- 资源耦合：将CPU、内存、硬盘等资源解耦合每一层单独调用，crash可以立刻从资源池寻找新的替代
- 弹性：即插即用，便于扩展
- 规模化
- 执行计划、分配
- 数据读写、共享存储



share nothing，通过网络传输

带来的问题：

TCP/IP传输，会进行packet的封装，导致CPU负荷过大

现在出现了新的技术，不经过OS、kernel，直接把包给到网卡，然后发出去，很快

网卡、路由器、交换机等网络设备需要支持RDMA，通过高性能网络访问其他设备上的数据



### 计算下推

云原生还有一个特点：计算下推

云原生的计算节点和存储节点是分离的，缺点是存储节点的计算资源被浪费了

而且存储节点会把大量数据发送给计算节点，导致内部的带宽占用很大

如何利用存储节点的计算资源？

计算下推！

在进一步思考，能不能下推到存储介质上边？

总之没最终的目的就是，通过不断地计算下推，减少内部的数据流动，分摊压力，充分利用计算资源

可惜的是，上述的设计是没有统一的标准的，需要结合硬件资源和特殊的设计才可以完成，这也是云原生强大的特点



视图：

物化视图在cache里边，存一下元数据和哪一个物化视图最匹配

物化视图存在重复数据，可以进行一些集合操作，不用加载那么多



### 数据仓库

当业务变大，会产生其他系统

如果进行数据分析，可能对多个主题进行操作



那么如何对不同数据库的数据做一个拼接耦合？

1. - ETL：数据的清洗、转化、加载
   - 多个数据源的数据首先进行一次转换
   - 数据在加载时，可能需要进行某些语义、数值、类型的处理
   - 数据清洗，删除dirty data
2. - 数据建模，按照OLAP的需求进行建模
3. - 进行分析、可视化



在数据仓库中，主要针对的时OLAP的处理，就是为了做分析的。数据仓库的数据源是非常多的



shard server的模式是share nothing，并行处理性能很好，但是一旦涉及到互相之间的交互，就寄了



### 数据湖

数据湖中的数据时存在各种格式的，不需要做ETL

进行元数据管理

进行分析时在把数据从数据湖抽取出来，导入到数据仓库



### 湖仓一体

当所有数据都被数据仓库用到时，就等于说数据湖全部数据被复制了一遍，等于多做了一次复制

因此有人提出湖仓一体的概念

而且根据数据的类型，在进入时就可以决定放在数据湖还是数据仓库



### 对比数据仓库和数据湖

| 数据仓库                         | 数据湖                                     |
| -------------------------------- | ------------------------------------------ |
| 数据体系严格，提前建模（OLAP）   | 数据体系松散，事后建模                     |
| 灵活性较低                       | 灵活性较高                                 |
| 数据治理容易                     | 数据治理困难                               |
| 数据种类单一（结构化、半结构化） | 数据种类丰富（结构化、半结构化、非结构化） |
| 面向成熟数据的企业级分析与处理   | 面向异构数据的科学探查与价值挖掘           |
| 向特定引擎开放，易获得高度优化   | 向所有引擎开放，各个引擎有限优化           |



## 11.25 Clustering

### 会话粘滞性

load-balance策略：

考虑连接数

最简单的是round-robin轮询

可以改变weight

如果是按照IP的hash，可以解决会话粘滞性的问题，但是不能做到负载均衡



解决会话粘滞性的问题还可以使用第三方工具存储session

比如，用redis存储session，这样集群服务器都会向redis取session

- 缺陷是redis单点故障：可以备份
- 向redis取session会稍微慢一点



### 数据库集群

MySQL主从备份

至少有三个节点

MySQL集群的作用：

- 可以做数据备份，如果主节点炸了，不至于数据全都没有
- 从节点可以分担读请求，但是写请求还是只能在主节点上边响应
- 设置好种子，集群可以动态增加



主从备份：

- cold：只写一个，每隔一段时间进行数据同步
- warm：每隔一小时更换主从节点的角色，进行切换
- hot：拿到请求，各处理各的，同时进行



还可以是多个primary



## 11.29 虚拟化 Virtualization

JVM把bytecode翻译成机器可以听懂的东西，Java可移植性比较好



VM：对CPU和内存做直接的虚拟化，对IO操作经过特殊的虚拟机转化为对物理机的操作



Image文件切分，如果Image文件有相同的部分，则可以对相同的部分进行复用

Docker+K8S

KVM+Open Stack

监控的数据都放在时间序列数据库中



虚拟机运行的是完整的操作系统。虚拟机的目的是做隔离，但是跑完整的操作系统占用空间太大了

docker是把对linux的的调用转换为对物理机操作系统的调用



containr是一个运行时。容器就是把你写的代码和需要的东西全部打包运行

docker是分层打包的

docker的image没有包含完整的操作系统

拿一个Image可以运行多个实例，App的系统调用发送给docker引擎，转换为host machine的系统调用

容器互相之间是隔离的，每个容器都有自己独立的IP

可以通过IP加端口访问container，也可以通过端口映射访问container

一个docker不能直接访问另一个docker的内容，必须通过网络进行访问



*（这一段是在讲什么啊我焯）*

*java代码需要JVM来加载，JVM是用java写的*

*最初的ClassLoader是用C写的*

*在打包时，不应该把MySQL驱动放在/lib/ext中，不要直接打包*

*ClassLoader也是一种隔离*



docker run的一些参数

- -d 后台运行，守护进程
- -p 端口映射 当前机器的xxxx端口映射道docker container的yyyy端口



内容持久化container之间的内容是完全隔离的

写入container的文件，重新启动一个ubuntu的实例，是没有之前写入的文件的，不会保存在Image中

能否持久化？Volume！

可以将volume mount到container中（文件映射）



通常volumn是一个文件，但是：

- 挂载数据库？
- 远程文件？



是可以挂载远程目录的

同一个Image在跑不同的实例可以挂载不同的volume，就不会相互干扰



## 12.02 Docker & K8S

docker的image打包了一些依赖的库，将自己的应用打包进去，变成一个容器，交给Docker Engine运行

container的调用全部交给docker engine，docker engine转换为对host OS的调用



是否可以把NodeJs和MySQL搞成一个缝合怪？

不太好，一个崩了另一个也崩了



而且，Image是分层的，即使是不同的Image，也有可能是某些层是完全相同的，在pull的时候已经有了分层，再缝合不太好



挂一个卷进去，当一个容器销毁的时候，它的数据可以在本地存着，不会顺带销毁



### Docker Compose

每次启动两个容器，好复杂，能不能有方便的方法？

Compose！

需要写Compose文件，放在目录里边



### K8S

docker有一个运行时，可以把docker的底层运行时换掉，换成更加轻量级的

K8S是用GO语言写的，如果想魔改，需要用GO语言编写

在一个集群里边，如果每一个都可以跑后端，如果发现container崩了，需要人为进行重启。

有没有什么自动化的东西，可以进行自动部署和迁移？

这一系列的东西使用K8S可以搞！

K8S：

- 服务发现和负载均衡
  - 所有的容器在K8S注册一下，还可以进行负载均衡
  - 因为位置是主动注册的，K8S知道容器在什么位置，无论IP、端口怎么变化，都可以进行负载均衡（因为是按照名字找的）
  - 一个崩了，可以进行迁移，这个过程用户是无感的
- 还能进行存储编排
  - 不同的容器可以挂载不同的卷，进行存储编排
- 状态回滚
  - 可以检查状态是不是正常
- 部署，背包问题
- 自愈
  - 可以关掉，再启动一个
- 安全问题



控制面：要完成的所有功能

控制面包含的构建很多，建议把控制面装在一台机器上，不要和容器节点放在一起

- API-server：类似gateway，因为它可能是系统瓶颈，可以集群化部署

- 调度器：判断容器如何分配节点，调度器可以跑多个实例
- ETCD：状态存储库，不能是集群，只能是一个地方完整地存储
- 容器管理器：
- 云相关的容器管理器



Node：容器节点

- 在每一个节点上都要安装一个kubelet小程序，监控节点的状态
- 节点上还要有k-proxy，使节点在同一个网络内互通



不能直接房屋内node，需要访问api-server

pod是一个server+mysql，这是部署的一个实例

pod是部署的最小粒度，容器的最小单元，其中的server和mysql不能拆开



## 12.06 云计算

云计算对应了边缘计算，所以有人想把边缘计算叫做雾计算



云计算需要进行

- 操作系统的调度
- 分布式文件系统
- 数据库
- 数据库上的事务管理



这些和单机版本有什么区别？



云计算之前有一种叫做网格计算：

- 所有人计算资源共享
- 没有得到人们的响应



云计算：

- IBM看见了商机：厂商有大量资源，拿出来给别人用，这些机器看不见，就好像在云上
- 其实就是把集群共享出来，供别人使用
- 暴露的是服务，web服务，这些服务都不在本地，而是通过互联网去访问的
- 云服务是按需计费、弹性拓展的
- 云服务的所有管理上的复杂性都是云服务厂商管理的



需求不同，如何应对复杂的、不同的需求？

- 需要进行**虚拟化**，服务器全部装同样操作系统，操作系统上边运行不同的虚拟机
- 哪怕规定只能用linux操作系统，也必须虚拟化
- 原因是，为了满足需求，云服务厂商购买了16核100G内存的高性能服务器，而如果用户只要2核8G内存，需要进行资源分配，这也需要通过虚拟化实现
- 虚拟机之间是隔离的，用户看不到其他的虚拟机
- 像JDBC一样，少量的资源能干很多事情，原因是用户A租了一台云服务器，但是这台服务器在不忙的时候会被资源回收，资源分给其他的服务器使用



云服务的特点：

- 弹性定价策略

- 弹性拓展，能用eureka进行服务注册
- 快速供给，我买了之后一分钟给我
- 虚拟化



云服务的必要支持：

- 负载优化
  - 监控机器的占用情况，把服务全部迁移到一部分机器上边
  - CPU调频，节约电
- 整体的服务控制、调度
- 部署选择
  - 共有云还是私有云



### 云计算的核心技术

核心技术

- 调度系统：代表性的是MapReduce
- 分布式文件系统：代表性的GFS
- 分布式存储系统：代表性的BigTable
- 内存管理：有Xen等



MapReduce：

- mapper处理数据，达到阈值之后写文件
- shuffle把mapper产生的数据进行排序和分组（把每个mapper的文件变为每个reducer一个，便于reducer处理）
- MapReduce是有大量硬盘读写的系统



Spark：

- 由于MapReduce磁盘IO很多，速度很慢，Spark就想能不能把操作都放在内存里边运行
- Spark的要求：首先需要内存大一点
- 其次Java就不行了（Java太吃内存），换成Scala，而且进行函数式编程



边缘计算：

- 雾计算，在身边的
- 云计算，放在远端、看不见的
- 软件学院摄像头的例子，通过摄像头的计算资源进行匹配
- 把计算推到网络的边缘，从终端到云，在任何一个节点都可以做处理
  - 数据永远在靠近数据的地方处理是最快的
  - 移动网络是不可靠的网络，不保证一直是持续的



下述讨论的是面临移动网络下的场景，最大的问题是数据传输的速度，移动网络（主要设备是手机）带宽是非常不稳定的。而且在移动，可能刚刚连接上第一个基站，这个基站开始运行任务；移动至第二个基站时，第一个基站就没法连接上了。运营商需要做虚拟机的迁移，把第一个基站的虚拟机迁移到第二个上边。



边缘计算解决的问题：

- 数据的处理要靠近数据的源头，就近存储、就近处理
- 移动网络，不可靠的网络连接，可能不能连接到云端，只能连接到基站



任务的本地执行、部分卸载、完全卸载（offloading）

- 本地能不能做？不能做就丢到上一层
- 能做一部分，可以把一部分丢到上边
- 不至于从小基站到宏基站一条路带宽全部占满



## 12.09 Hadoop

核心时要跑一个分布式文件系统，后台有一个yarn调度这个分布式文件系统



combiner只处理一个mapper上边的输出，将一个mapper里边的数据进行初步的合并。

那么还要不要reducer吗？需要的，因为reducer可能收集来自很多个mapper的数据，combiner只是减少数据冗余，做一个初步的处理



map的数量和文件数量、大小有关，是通过Hadoop的框架进行调度的

combiner是复用的reducer的代码，相当于是在本地做的reduce，combiner的数量和mapper有关

reducer的数量是可以人为控制的，如果不设置，会根据CPU核数和任务的情况进行设置



如何扩展数据：

- 如果输入文件太大，MapReduce会把文件切分成固定尺寸的split，一般是文件系统block size的大小，HDFS中的split size默认是64MB
- 每一个split会分配一个map任务
- spilt尺寸要合适，如果太小了，要做map太多次；如果太大了， 读的效率不一定搞
- Reducer也可以是多个，每一个Reducer只负责一部分



### 关于Job和Task的重要概念

Job和Task：

hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束



Job tracker的任务

1. 接受所有提交上来的任务
2. 看集群里边哪些机器是空的，然后给空的机器启动task，起到任务管理器的作用，管理哪些程序应该跑在哪些机器上
3. 还要跟踪每一个task是不是崩了，如果崩了要在另外的地方另外启动一个
4. Job Tracker在系统中只有一个，同时承担的任务过于重要，如果Job Tracker崩了，系统就完蛋了



Task Tracker：

1. Task Tracker是每个机器上都有的一个部分，他做的事情主要是监视自己所在机器的资源情况
2. TaskTracker同时监视当前机器的tasks运行状况，TaskTracker需要把这些信息通过heartbeat发送给JobTracker，JobTracker会搜集这些信息以给新提交的job分配运行在哪些机器上



- 一台机器上一般可以同时启动10-100个Mapper，不建议更多是因为机器上边的进程太多会导致进程调度困难

- MapReduce需要做一些任务的决策，考虑如何切分文件、分配任务，因此在数据量少的时候，额外开销是比较昂贵的

- reducer的数量：$number\ of\ nodes \times\ number\ of\ containers\ per\ node\ \times\ weight$，weight在0.95到1.75之间
- map主要设置是spill文件，内存满了就spill写到硬盘
- map把中间结果通过网络发送给reduce，为了减少尺寸，还需要进行压缩
- 还可以设置每spill多少应该进行一次spill文件的合并等
- Job Tracker



### Yarn

Yarn把资源管理器和任务管理器分开

每一个Job都有一个master，管理自己的mapper和reducer，就把应用的管理和资源管理分开。

client请求ResourceManager，每一个机器上都有NodeManager管理资源，并且将硬件资源的情况告诉Resource

而MapReduce的任务管理会针对每一个单独的Job启动一个ApplicationMaster，每一个AppMaster管理自己的Mapper的Reducer，当AppMaster收到情况之后，将消息传回给ResourceManager

ResourceManager也会根据硬件资源的占用情况，选择在适当的机器上启动AppMaster和Container（container就是mapper或者reducer）



## 12.13 Spark

MapReduce频繁地读写硬盘

读一个split，写满内存将spill文件写到硬盘。处理完要将小文件合并、排序，再传到Reducer上边



Spark就是为了解决MapReduce的这个缺点

几个主要的功能：

- 建立在HDFS之上，可以处理批数据/流数据
  - 流数据也是通过切分时间片的方式进行处理的
- 支持Spark SQL，但是底层不一定是结构化数据
- 大数据科学
- 机器学习库



特点：

- 速度快，所有操作都在内存中完成
- 可以集成分布式文件系统



RDD：弹性分布式数据集

每读一行，都是RDD的一个元素，但是RDD可以在几个机器上分布（在内存里），逻辑上是同一个



Spark的几个特点：

- 惰性操作，用到才会执行具体的transformation语句，产生新的RDD，非万不得已不要执行，否则占用内存
- cache操作：由于内存满了会根据LRU进行内存替换，t掉旧的内存，如果用了cache，说明这个message很重要，不会参与内存的替换
- cache只是一个想法，告诉Spark这部分数据需要cache住。如果cache过多，会进行一个压缩，如果压缩都不行，就放在硬盘上



在Spark上做的所有操作分为几类：

- transformation：变换，输入时RDD输出也是RDD，对RDD进行变换生成一个新的RDD，但是不会修改原来的RDD，而且不会立刻运行
  - map filter等
  - transformation可以在本地做，每一块只依赖于之前的一块的RDD，不需要和其他的进行交互
- action：立刻执行，不会等待，返回值不是RDD，会形成DAG图提交Spark集群运行，并立即返回结果
  - 依赖于多个块，不能在本地做



Spark的工作流程

- 所有的操作都是从Driver Program中的SparkContext开始的
- 将任务提交给Driver Program之后，SparkContext和Cluster Manager沟通，分给合适的Worker Node执行
- 每一个Worker Node都有一个Executor的进程，执行一些task，并将执行完的将结果给SparkContext
- Cluster Manager负责监控的作用，监控Worker Node的情况等。



文件是分布在不同的机器上，WorkerNode最好靠的比较近，而且要求Worker Node和Driver Program的网络是相连的



写好的代码不是在IDE里边直接跑，而是用过Spark的submit命令，把写好的东西扔到一个节点上跑



### RDD

- 可以并行处理的数据集
- 可以存在多个机器上，逻辑上是同一个，物理上可以分布式存储在内存中
- 容错性好，可以恢复
- RDD可以从现成的RDD产生或者从文件中读取
- RDD加载之后就需要进行分区
- RDD可以进行transformation和action，区别在上边说过
- 可以持久化（指的不是存到硬盘上，而是在cache里边持久存储）



partition：

- 如果没有认为控制，就需要问每一个机器这块数据你有没有，是很大的开销
- 如果做了partition，只需要找特定的机器就可以拿到数据



### 宽依赖/窄依赖

宽依赖以及窄依赖：

- ppt有例子

- 窄依赖：父RDD的每个分区只被子RDD的一个分区所使用
- 宽依赖：父RDD的每个分区都可能被多个子RDD分区所使用



宽依赖可能带来很多网络开销，如果一个RDD崩了，需要前边父RDD的所有分区才能生成，代价很高；如果是窄依赖，不需要很大开销

宽依赖往往对应着shuffle操作，需要在运行过程中将同一个父RDD的分区传入到不同的子RDD分区中，中间可能涉及多个节点之间的数据传输；而窄依赖的每个父RDD的分区只会传入到一个子RDD分区中，通常可以在一个节点内完成转换。



stage：

stage的切割规则：从后往前，遇到宽依赖就切割stage

调度时按照stage执行的，可以将stage的结果缓存下来，不用一直回推到最开始。遇到宽依赖不得不运行时，才去做一个stage



on heap和off heap

使用OFF_HEAP的优点：在内存有限时，可以减少频繁GC及不必要的内存消耗（减少内存的使用），提升程序性能



Spark SQL：

专门处理结构化数据，可以看作一个内存数据库，默认按照列存储



流式数据处理：

来了之后就插入到表中，像是一个没有边界的表，通过划分细粒度的时间片，把批处理变成类似流处理，每个细分的时间片内仍然是批处理



## 12.16 Storm

专门拿来做流式数据处理

由spout和bolt构成一张有向无环图DAG



处理的是没有边界的流数据

可拓展的（storm本身做不了，需要zookeeper支持拓展性、容错性）

zookeeper管理集群中的每一个节点（资源控制）

master在Storm中被叫做Nimbus

zookeeper怎么直到worker可用？和worker机器上的supervisor通过网络连接，supervisor将worker的信息告诉zookeeper

分布式环境最好把资源控制和任务调度分开，参考Hadoop的Yarn



Topology：

Storm中运行的一个实时应用程序的名称，将 Spout、 Bolt整合起来的拓扑图。定义了 Spout和 Bolt的结合关系、并发数量、配置等等

Spout：

在一个topology中获取源数据流的组件。通常情况下spout会从外部数据源中读取数据，然后转换为topology内部的源数据。

 Bolt：

接受数据然后执行处理的组件,用户可以在其中执行自己想要的操作。

 Tuple：

一次消息传递的基本单元，理解为一组消息就是一个Tuple。

 Stream：

Tuple的集合。表示数据的流向。



构建DAG的过程，有点类似于PyTorch里边构建神经网络的例子



## 12.20 HDFS

### 一些基本概念

HDFS不是从头开始搭建的，底层是linux文件系统

面向尺寸特别大的文件，一个大文件切分成HDFS的block size放在不同的机器上，不同机器并行读，就相当于增加了带宽；如果是小文件，只能在一个机器上，而且需要HDFS这一层直到文件位置再去向linux文件系统取文件，导致速度降低。



HDFS面向的应用场景是写入一次，多次读取的方式访问。如果是插入、更新，导致每一个block都要修改，开销巨大，而后边追加的操作时可以的。这种文件系统的应用场景很适合时序数据库



由于底层的机器是廉价机器，随时会崩溃，因此为了容错，数据需要有replica

Name Node和client交互，告诉client文件具体在哪一个Data Node上边，client直接和Data Node交互，client和Name Node只交互一次

当有大量的block存到一个Name Node上边，文件的路径是/hdfs/1.txt，但是当它向Data Node存储时，可能分块存储成为/hdfs/1-1.txt和/hdfs/1-2.txt，Data Node并不知道自己存的什么，也不知道自己存的文件的名字

Data Node会自己建一个目录，再去找具体的文件，这个目录和HDFS的目录没有任何关系，是内部自己组织数据的一种方式，是user-invisible的



一般来说Name Node单独用一台机器来跑，因为Name Node上边存了元数据，需要它的负载轻一些



HDFS也支持用户配额，限定用户最多使用多少数据

不支持链接和软链接



Name Node中记录的数据：

- 文件的名字
- 文件replica数量，不一定是全部相同的，可以指定副本因子
- 多少的block，id是什么，分布在哪些机器上



HDFS删除文件是先放到垃圾箱，过一定时间才彻底删除，因此刚刚删除文件时占用的空间并不会减



- Data Node发心告诉Name Node自己还或者
- 如果Data Node死了，就不在向他转发请求
- 会将死亡的Data Node上边的block进行拷贝，满足副本因子的要求



### 心跳

Block Report和心跳要分开，因为Block Report没必要太频繁

心跳不能太严格，如果在一定时间崩了才会判定死亡

为什么不能太严格：

- 如果是定时器，Timer是多线程，Timer线程正常不代表读写线程是正常的
- 需要用while循环，和业务在一个线程，业务死了，心跳肯定发不出去。但是如果这样，时间就不一定准，因此心跳不能太严格



### 机架感知

如何防止副本？一定要在不同的机架防止，提高容错

同时，如果距离太远，在Data Node写时，网络开销很大，需要做一些平衡

每一个机架的replica数量也有上限



### 安全模式

HDFS在启动时会检查block是否满足副本因子的要求，如果不满足会进行replica



### 元数据

edit log不能存在HDFS里边，因为是要靠edit log保证数据的原子性，因此edit log放在OS的file system中

FSImage也在OS的local File System中，FSImage存的是元数据

来了写操作，首先写edit log，写完edit log再写metadata

checkpoint在FSImage更新，edit log清空之后



### Failure

如果是Data Nodeheartbeat断了，过了指定时间，这个Data Node就被标记为死亡，在这上边的replica都无效了，这上边的副本因子就清空

如果说某些replica的checksum不符合标准，则Name Node就会提高副本因子，再做一遍replica，保证replica数量足够

数据完整性是通过checksum进行检验的，checksum不能和文件数据存放在一起，而是单独放在隐藏文件中

如果edit log和FSImage坏了，那么整个文件系统就不行了，因此它们也需要多个副本。这些副本不能放在HDFS中，而是应该放在本地文件系统中



### 文件删除

把文件放在trash目录中

目录移动也比较简单，因为Name Node不存放数据，改目录也只是改元数据，而且目录也只是一个逻辑概念，不是真正移动

因此文件删除这个操作非常快

调副本因子时，也只是把一些block放到trash中，这是由Name Node进行选择的



## 12.23 HBase

关键词：

- 大数据存储
- 分布式
- 带版本
- 面向列的
- 非关系型数据库，schema不严格



### 基本概念

HBase面向列的，有列族的概念

每一个cell都有时间戳，带有版本的概念，如果查找是没有特殊要求，默认返回最新版本的值

在硬盘上按照region的模式存，一张表的一些行作为一个region，但是存储模式是按照列存储，由于region的模式，可扩展性很好，一个region满了再开一个region，一个region server满了再开一个region server

按列存储，可以更改编码机制，更加省空间，压缩机制上，可以进一步压缩

按照id进行排序，然后将排序后的数据存放在不同的region中



HBase没有库的概念，取而代之的是Namespace

其次就是行和列的概念

列被组织成了列族，存储的是相关的列

在设计HBase的表时，需要改变关系型数据库的设计思路，把原来的整个关系型数据库数据库存成一张表，把原来觉得比较内聚、在关系型数据库总是一张表的数据放在一个列族中。

列族内部的数据是存在一起的，列族之间是分开的

HBase会自动进行表的切分，当一张表大到一定程度会自动切开，水平切分产生另外一个region

HBase表的删除需要先disable掉，否则不能删除

建议列族不要超过三个，一旦列族太多，需要记住在一个region中列族在什么位置，列族多了index就很长

```
put	'test', 'row1', 'cf:a', 'value1'
put	'test', 'row2', 'cf:b', 'value1'
put	'test', 'row3', 'cf:c', 'value1'
```

|      | cf:a   | cf:b   | cf:c   |
| ---- | ------ | ------ | ------ |
| row1 | value1 |        |        |
| row2 |        | value2 |        |
| row3 |        |        | value3 |

稀疏的表，而且是动态插入，一开始并没有这三行



HBase物理上的存储：

row key + time stamp + content



版本号可以指定最多存储几个，在取数据时，如果不指定，默认取最新的数据



有一些关键点：

- region尺寸，不要太小，不到一个block很浪费，太大一台server放不下
- cell不要太大
- 列族最好一到三个
- region的数量最好在50-100
- 列族名字短一点，因为列族要参与索引，长了占用空间
- 如果是时序数据，主键设计比较重要
- 操作一张表的时候只允许有一个列族正在运行



## 12.27 Hive

Hive数据仓库

用了很多数据库：

- MySQL
- MongoDB
- Neo4j
- InfluxDB
- RocksDB
- ……



从业务需求，可能需要把所有数据放在一起做分析，需要导入数据仓库

Hive里边用Ctrl+A把数据断开，或者指定几类格式文件，只要复合规定就可以导入数据仓库

导入数据仓库之后，变成一张一张的表，导入之后对DataWarehouse就没有任何区别了

导入数据仓库要经过数据的过滤、清洗、转换、加载的过程



如果是schema on write，在写入的时候校验数据是否符合格式，这样要过很久才能告诉client是否加载成功，而且这张表甚至不会使用，因此数据仓库采用Schema on write开销比较大



### 数据仓库的一些讨论

但是什么时候做数据过滤清洗、转换、加载的过程？

- 数据仓库是Schema on read，只有在使用数据时才会进行数据的校验
  - 数据仓库在load数据之后，原始数据还在原来的库里，处于分离的状态，就没有原始的数据了（原始的格式没了，在数据仓库上边是去掉了原始格式的一张一张的表），而数据湖存储的时原始的数据（原始格式）
- 如果只使用数据湖：可以将数据湖的各种原始数据搭配上SQL的转换器，这样就将数据湖当作数据仓库使用了，不需要数据仓库了，但是性能必然没有数据仓库好
- 湖仓一体（LakeHouse）：
  - 热数据在数据仓库，冷数据在数据湖，如果数据湖中的数据被多次访问，就放到数据仓库中，这种设想是make sense的



### Hive

目标是做大数据的分析，是Hadoop的一个组件，底层是HDFS

数据加载进Hive中之后，让熟悉SQL的人可以用类SQL的语句操控这些数据进行大数据分析

类SQL语句是用MapReduce运行的



/user/hive/warehouse用于存储元数据（表的结构等……）

/tmp目录用于存储临时文件，因为load数据要进行一些过滤、清洗、转换，会有中间数据生成



Hive里边按某一列做Partition



Hive加载数据，采用MapReduce的方案，同样地Hive几乎所有的SQL都是MapReduce的模式执行的



频繁初始化HDFS会造成Name Node和Data Node不一致



Hive支持的数据格式包括纯文本文件、Parquet、ORC等格式，因此大家可以把数据都导入进来



RCFile：

先水平切成很多块，而对于每一块，是按列存储的。数据从数据库中转换为RCFile，从而load到数据仓库中

还有Parquet，也是数据库转成Parquet在导入Hive中

这就是为什么Hive使用schema on read，不然laod太慢了



Hive还可以直接加载压缩文件到某一个表中，等待以后实际用到才会解压缩，这样子空间占用会比较小，但是由于原始数据没有还原出来，不能进行数据的切分，就只能有一个Map和一个Reduce，不能并行操作，时间占用比较大



## 12.30 Put them all together

A问B请求，不想要数据的结构，想要的是高级信息，比如今年的业绩blabla



烟囱：各个部门、公司之间孤立的数据系统

因此要构建数据中台：

- 聚合、治理跨域的数据，封装成服务，供前台业务调用
- 企业内外部异构的数据采集、治理、建模、分析、应用，使数据对内优化体改业务，对外可以使数据合作价值释放，成为企业资产管理中枢



层次划分：

1. 数据层：高压数据流，数据接入接口
2. data hub数据集成
3. 计算层：数据计算引擎（比如AI模型等），大数据平台（允许拿到集成完的数据），云平台数据库
4. 服务层：搞搞服务什么的



数据中心建设思路：

1. 数据清洗与转换
2. 数据弹性存储
3. 数据融合（基于并行处理基础设施）
4. 面向应用的数据服务（最终给用户暴露的东西）



key points：

1. **统一时空**基准下的数据融合（时间：统一授时，空间：空间基准相同，就好像是对空间的描述）
2. 面向xxxx的数据架构（物理实体：船，虚拟实体：航线），搞搞“数字孪生”
3. 精细化的数据治理（统一数据表征）
4. 弹性存储与处理计算基础设施



- 数字化（把东西存在电脑里），数据化（做过处理，能够进行一些操作）

- 物理世界的数字化
- 数字资产化、资产数字化赋能平台



### 总结一下！

#### 服务器端开发

- 大二的时候，tomcat访问mysql，解决了结构化数据的存储
- tomcat是java的一个进程，要控制实例数量
- Scope控制实例数量，实质是节约内存资源
- tomcat访问mysql，一般是同步调用，导致阻塞。大量请求，可以改用异步的通信方式：发消息。记下来之后说我收到了，事后处理。但是事后处理的前提是，有空闲时候，可以做到削峰填谷。如果一直很忙，全是峰，就寄了。

#### 事务

事务用描述性的方式

requires_new不是嵌套事务，根本不是一回事

同步，用锁

wait-notify

cache

异构，WebService，SOAP

WebService里边为什么要用微服务

微服务的各个服务之间比较独立，一个崩了其他的没事，而且可以独立地加资源

所有服务可以启动多个实例，但是写好的代码不可能知道各个实例在哪里，因此有一个注册中心

用户发请求到GateWay，然后GateWay在注册表上边找一下，进行调度

好了，你的应用现在很牛逼了！

#### 数据库

关系型数据库四节课

MySQL缓存-打开的表有哪些，索引有哪些

缓存大小有限制



备份：

- 集群主从备份

- 逻辑备份bin log，可以跨版本甚至跨数据库
- 直接拷贝文件做物理备份，比bin log省空间，但是会有版本问题
- partition，判断做partition有没有好处



讲完MySQL拓展出了一堆其他东西



MongoDB

- 面向document
- 表叫做collection
- 同一个collection的schema可以不一样
- MongoDB的sharding根据key进行sharding



Neo4j：

- 图数据库
- 好友关系，社交网络



LSMT InfluxDB：

- WAL
- LSMT没有空间放大，但是有读放大：全部读完才能判断有没有，写放大：有可能很多compaction



Datalake：

- 数据库很多，怎么放在datalake里边



当太多的时候，做分布式集群，集群的会话状态维护：

- gateway+register
- nginx



docker：

- 每一个docker有独立的ip
- 容器化部署
- 要有一个容器管理的功能，K8S



如果在云里边：

- cloud computing
- edge computing



hadoop：

- MapReduce并行计算，但是效率不高，读写硬盘太多



Spark：

- 不读写硬盘，进行内存操作
- RDD，分布式内存



storm：

- 上边的都是批数据
- 流式处理用storm



HDFS：

- 本地文件系统上边构建出HDFS
- 在HDFS上构建出HBase数据库和Hive数据仓库
- HBase可以数据切分
- Hive在load时候不做校验，而是在做sql式校验
